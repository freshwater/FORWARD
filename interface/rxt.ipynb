{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tf\n",
    "import PIL.Image\n",
    "\n",
    "kappa = PIL.Image.open('data/kappa.png')\n",
    "kappa_tensor = tf.to_tensor(kappa)[1]\n",
    "\n",
    "j = PIL.Image.open('data/J.png')\n",
    "j_tensor = tf.to_tensor(j)[1]\n",
    "\n",
    "dd = PIL.Image.open('data/dd.png')\n",
    "dd_tensor = tf.to_tensor(dd)[1]\n",
    "\n",
    "maps = PIL.Image.open('data/maps.png')\n",
    "maps_tensor = tf.to_tensor(maps)[1]\n",
    "\n",
    "image_shape = torch.tensor([34, 28])\n",
    "image_size = image_shape[0] * image_shape[1]\n",
    "offset_size = image_shape[1].item()\n",
    "\n",
    "classes_count = 2\n",
    "\n",
    "class Kappa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Kappa, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(image_size, classes_count)\n",
    "        self.memory = torch.nn.Parameter(torch.zeros(classes_count, image_size))\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(image_size + classes_count, offset_size)\n",
    "        self.offsets = torch.stack([\n",
    "                           torch.tensor(range(image_size)).reshape(*image_shape).roll(i, 1).flatten()\n",
    "                           for i in range(offset_size)])\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = image.view(-1, image_size)\n",
    "        x = self.fc1(x)\n",
    "        class_ = F.softmax(x)\n",
    "        \n",
    "        recalled_image = class_ @ self.memory\n",
    "        \n",
    "        z = image.view(-1, image_size)\n",
    "        z = torch.cat([z, class_], 1)\n",
    "        z = self.fc2(z)\n",
    "        offset = F.softmax(z)\n",
    "        \n",
    "        recalled_offset_image = []\n",
    "        for choice, image in zip(offset, recalled_image.reshape(-1, image_size)):\n",
    "            recalled_offset_image.append(choice @ image[self.offsets])\n",
    "        recalled_offset_image = torch.stack(recalled_offset_image)\n",
    "        \n",
    "        # recalled_offsets = recalled_image.T[self.offsets].T\n",
    "        # print(recalled_offsets.shape)\n",
    "        # recalled_offset_image = torch.bmm(F.softmax(z).reshape(-1, 1, offset_size), recalled_offsets)\n",
    "        \n",
    "        return class_, offset, recalled_offset_image.view(-1, *image_shape)\n",
    "        \n",
    "    def determine_class(self, image):\n",
    "        class_, offset_, recalled = self.forward(image)\n",
    "        return class_\n",
    "        \n",
    "        x = image.view(-1, image_size)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return F.softmax(x)\n",
    "\n",
    "    def determine_offset(self, image):\n",
    "        class_, offset, recalled = self.forward(image)\n",
    "        return offset\n",
    "        \n",
    "        class_ = self.determine_class(image)\n",
    "        z = image.view(-1, image_size)\n",
    "        z = torch.cat([z, class_], 1)\n",
    "        z = self.fc2(z)\n",
    "        return F.softmax(z)\n",
    "\n",
    "def train(model, device, optimizer, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    images = torch.stack([j_tensor, kappa_tensor]).to(device)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        indices = torch.randint(classes_count, size=(16,))\n",
    "        # indices = torch.randint(2, size=(16,))\n",
    "        data, target = images[indices], images[indices]\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            offset_in = torch.randint(image_shape[1], size=(1,)).item()\n",
    "            data[i] = data[i].roll(offset_in, 1)\n",
    "            target[i] = target[i].roll(offset_in, 1)\n",
    "    \t\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # log_choice, recalled_image = model(data)\n",
    "        class_, offset, recalled_image = model(data)\n",
    "        classification_loss = F.cross_entropy(class_.log(), indices)\n",
    "        memory_loss = F.mse_loss(recalled_image, target)\n",
    "        \n",
    "        # loss = classification_loss + memory_loss\n",
    "        loss = memory_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = Kappa().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train(model, device=device, optimizer=optimizer, epochs=2*2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.determine_class(torch.stack([j_tensor, dd_tensor, maps_tensor]))\n",
    "\n",
    "(100*results).round() / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m retro.import roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tf\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_codes = {\n",
    "    'UP':        [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'DOWN':      [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'LEFT':      [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'RIGHT':     [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    'NONE':      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'B':         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'A':         [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "actions = list(actions_codes.keys())\n",
    "\n",
    "codes_actions = {tuple(code): action for action, code in actions_codes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    def __init__(self):\n",
    "        self.environment = retro.make(game='SuperMarioBros-Nes')\n",
    "        # self.environment = retro.make(game='NinjaGaiden-Nes')\n",
    "        # self.environment = retro.make(game='Airstriker-Genesis')\n",
    "        \n",
    "        self.blocks_seen = []\n",
    "        self.blocks_seen_urls = []\n",
    "        \n",
    "        # self.frames_all = []\n",
    "        self.actions_all = []\n",
    "\n",
    "        self.encodings = set()\n",
    "        self.encodings_frame = set()\n",
    "        \n",
    "        asymmetric = torch.linspace(0.5, 1.5, 16*16*3)**3\n",
    "        asymmetric = asymmetric.numpy()\n",
    "        \n",
    "        aa = torch.tensor(asymmetric).reshape(16, 16, 3).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        filter_ = torch.tensor(aa, dtype=torch.float)\n",
    "        self.filter_ = filter_ / 16 / 16 / 3 / 255\n",
    "\n",
    "        self.frame = self.environment.reset()\n",
    "        # self.frames_all.append(self.frame)\n",
    "\n",
    "\n",
    "    def step(self, action, commitment_interval):\n",
    "        for _ in range(commitment_interval):\n",
    "            self.frame, reward, is_done, information = self.environment.step(action)\n",
    "        \n",
    "        # self.blocks_identify(self.frame)\n",
    "        # self.frames_all.append(self.frame)\n",
    "        self.actions_all.append(action)\n",
    "\n",
    "        return self.frame, reward, is_done, information\n",
    "\n",
    "    def close(self):\n",
    "        self.environment.render(close=True)\n",
    "        self.environment.close()\n",
    "\n",
    "    __del__ = close\n",
    "\n",
    "    \n",
    "    def blocks_identify_all(self):\n",
    "        t = torch.tensor(np.stack(self.frames_all)).float()\n",
    "        images = t.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # id_ = str(uuid.uuid4())\n",
    "        # np.savez_compressed(f'/tmp/{id}.npz', np.stack(self.frames_all).astype('uint8'))\n",
    "\n",
    "        output = F.conv2d(input=images,\n",
    "                          weight=self.filter_,\n",
    "                          stride=16)\n",
    "\n",
    "        output = output[:, :, 4:-1]\n",
    "        \n",
    "        return output[:-1], self.actions_all\n",
    "    \n",
    "    def frame_encode(self, frame):\n",
    "        t = torch.tensor(frame).unsqueeze(0).float()\n",
    "        images = t.permute(0, 3, 1, 2)\n",
    "\n",
    "        output = F.conv2d(input=images,\n",
    "                          weight=self.filter_,\n",
    "                          stride=16)\n",
    "\n",
    "        output = output[:, :, 4:-1]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_play(step_count, agent, return_frames=False, random_seed=None):\n",
    "    environment = Environment2()\n",
    "\n",
    "    frames_all = torch.zeros(step_count + 1, 224, 240, 3, dtype=torch.uint8)\n",
    "    # frames_all = np.zeros((step_count + 1, 224, 240, 3), dtype=np.uint8)\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    frame = environment.frame\n",
    "    frames_all[0] = torch.tensor(frame)\n",
    "    for index in range(step_count or 999999999):\n",
    "        if agent:\n",
    "            encoding = environment.frame_encode(frame).flatten().unsqueeze(0)\n",
    "            action = agent.predict(encoding)[0]\n",
    "        else:\n",
    "            action = actions[random.randint(0, len(actions)-1)]\n",
    "            # action = actions[4]\n",
    "        \n",
    "        action_code = actions_codes[action]\n",
    "        frame, reward, is_done, information = environment.step(action_code, 6)\n",
    "\n",
    "        frames_all[index + 1] = torch.tensor(frame)\n",
    "\n",
    "        if information['lives'] == 1:\n",
    "            break\n",
    "        \n",
    "    environment.close()\n",
    "    \n",
    "    # encodings, actions_all_codes = environment.blocks_identify_all()\n",
    "    # actions_all = [codes_actions[tuple(action_code)] for action_code in actions_all_codes]\n",
    "    \n",
    "    result = {\n",
    "        # 'EncodingsUniqueCount': len(encodings.unique()),\n",
    "        # 'FramesCount': len(encodings),\n",
    "        # 'Encodings': encodings,\n",
    "        # 'Actions': actions_all,\n",
    "        'FrameLast': frame\n",
    "    }\n",
    "    \n",
    "    if return_frames:\n",
    "        # return {**result, **{'Frames': environment.frames_all[:-1]}}\n",
    "        return {**result, **{'Frames': frames_all}}\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index in range(1):\n",
    "    result = generate_play(step_count=1600, agent=None, return_frames=True)\n",
    "    # print(result['EncodingsUniqueCount'], result['FramesCount'])\n",
    "    plt.imshow(result['Frames'][-3]);\n",
    "    frames_acc.append(result['Frames'])\n",
    "    print(f\"{index}.\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = (pyspark.sql.SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.executor.memory\", \"200g\")\n",
    "         .config(\"spark.driver.memory\", \"200g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\",\"200g\")\n",
    "         .config(\"spark.driver.maxResultSize\", \"200g\")\n",
    "         # .appName('lecture')\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(frames, width):\n",
    "    frames = frames.permute(0, 3, 1, 2).float()\n",
    "    # frames = frames.to(torch.float32)\n",
    "\n",
    "    unfolded = F.unfold(input=frames,\n",
    "                        kernel_size=(width, width),\n",
    "                        stride=width)\n",
    "\n",
    "    imagez = unfolded.permute(0, 2, 1)\n",
    "    \n",
    "    row = 4\n",
    "    imagez = imagez[:, row*15:-1*15]\n",
    "    imagez = imagez.reshape(-1, 210 - row*15 - 1*15, 3, width, width).permute(0, 1, 3, 4, 2)\n",
    "    imagez = imagez.to(torch.uint8)\n",
    "\n",
    "    imagez = imagez.reshape(-1, width, width, 3).unsqueeze(0)\n",
    "    \n",
    "    asymmetric = torch.linspace(0.5, 1.5, 16*16*3)**3\n",
    "    asymmetric = asymmetric / 16 / 16 / 3 / 255\n",
    "    \n",
    "    unique = torch.unique(imagez, dim=1)\n",
    "    unique = unique.squeeze()\n",
    "    # encodings = unique.reshape(-1, 16*16*3).float() @ asymmetric\n",
    "\n",
    "    # z1 = [Image(image, display_scale=2) for image in unique]\n",
    "\n",
    "    # sorted_ = sorted(zip(encodings, z1), key=lambda x: x[0])\n",
    "    # z1 = [b for a, b in sorted_]\n",
    "\n",
    "    # import pickle\n",
    "    # pickle.dump([len(z1), z1], open('data/active.pickle', 'wb'))\n",
    "    \n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = generate_play(1600, agent=None, return_frames=True)['Frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition(frames, 16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(1):\n",
    "    frames = sc.parallelize([random.randint(0, 9999999999999999) for _ in range(60)]) .map (\n",
    "        lambda x: partition(generate_play(1600, agent=None, return_frames=True, random_seed=x)['Frames'], 16)\n",
    "    )\n",
    "\n",
    "    frames2.extend(frames.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1940\n",
    "# 10-7261\n",
    "# 40-17073\n",
    "# 80-30214\n",
    "# 128-32026\n",
    "# 160-38604\n",
    "# 180-38207\n",
    "# 240-42112\n",
    "# 300-46418\n",
    "# 360-50823\n",
    "# 420-53841\n",
    "# 480-55910\n",
    "# 540-59669\n",
    "# 600-62157\n",
    "# 660-64616\n",
    "# 720-66916\n",
    "# 780-69911\n",
    "# 840-71086\n",
    "# 900-72458\n",
    "# 960-73631\n",
    "# 1020-74999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(frames2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(torch.cat(frames2), open('data/frames2_1720295_16_16_3.tensor.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "frames2 = pickle.load(open('data/frames2_16_16_3.tensor.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(blocks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward import *\n",
    "\n",
    "asymmetric = torch.linspace(0.5, 1.5, 16*16*3)**3\n",
    "asymmetric = asymmetric / 16 / 16 / 3 / 255\n",
    "\n",
    "uniques = torch.unique(torch.cat(frames2), dim=0)\n",
    "encodings = uniques.reshape(-1, 16*16*3).float() @ asymmetric\n",
    "\n",
    "z1 = [Image(image, display_scale=2) for image in uniques]\n",
    "\n",
    "sorted_ = sorted(zip(encodings, z1), key=lambda x: x[0])\n",
    "z1 = [b for a, b in sorted_]\n",
    "\n",
    "import pickle\n",
    "pickle.dump([len(z1), z1], open('data/active.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames2), len(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = torch.unique(torch.cat(frames2), dim=0)\n",
    "encodings = uniques.reshape(-1, 16*16*3).float() @ asymmetric\n",
    "\n",
    "sorted_ = sorted(zip(encodings, uniques), key=lambda x: x[0])\n",
    "z1 = [b for a, b in sorted_]\n",
    "z1 = torch.stack(z1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = z1.reshape(1, -1, 16*16, 3)\n",
    "\n",
    "pickle.dump([len(ims), [Image(im) for im in ims]], open('data/active.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "57599232/(16*16*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# frames = result['Frames'][-3]\n",
    "# frames = torch.tensor(frames).unsqueeze(0)\n",
    "\n",
    "# frames = torch.tensor(result['Frames'])\n",
    "# frames = result['Frames']\n",
    "print(frames.shape)\n",
    "frames = frames.permute(0, 3, 1, 2)\n",
    "print(frames.shape)\n",
    "frames = frames.to(torch.float32)\n",
    "\n",
    "unfolded = F.unfold(input=frames,\n",
    "                    kernel_size=(16, 16),\n",
    "                    stride=16)\n",
    "\n",
    "print(\"unf\", unfolded.shape)\n",
    "\n",
    "imagez = unfolded.permute(0, 2, 1)\n",
    "print(\"perm\", imagez.shape)\n",
    "row = 4\n",
    "imagez = imagez[:, row*15:-1*15]\n",
    "imagez = imagez.reshape(-1, 210 - row*15 - 1*15, 3, 16, 16).permute(0, 1, 3, 4, 2)\n",
    "imagez = imagez.to(torch.uint8)\n",
    "\n",
    "imagez = imagez.reshape(-1, 16, 16, 3).unsqueeze(0)\n",
    "\n",
    "print(imagez.shape)\n",
    "\n",
    "# z1 = [[Image(im, display_scale=2) for im in frame] for frame in imagez]\n",
    "\n",
    "import pickle\n",
    "# pickle.dump(z1, open('../interface/data/active.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asymmetric = torch.linspace(0.5, 1.5, 16*16*3)**3\n",
    "# asymmetric = asymmetric.reshape(16, 16, 3)\n",
    "asymmetric = asymmetric / 16 / 16 / 3 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from forward import *\n",
    "\n",
    "unique = torch.unique(imagez, dim=1)\n",
    "unique = unique.squeeze()\n",
    "# unique.sort()\n",
    "print(unique.shape)\n",
    "encodings = unique.reshape(-1, 16*16*3).float() @ asymmetric\n",
    "\n",
    "z1 = [Image(image, display_scale=2) for image in unique]\n",
    "\n",
    "sorted_ = sorted(zip(encodings, z1), key=lambda x: x[0])\n",
    "z1 = [b for a, b in sorted_]\n",
    "\n",
    "import pickle\n",
    "pickle.dump([len(z1), z1], open('data/active.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
